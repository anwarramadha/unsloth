#!/usr/bin/env python3
"""
Simple inference script using SahabatAI (without RAG)
Based on the official example
"""

import torch
import transformers

# Configuration
MODEL_ID = "GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct"
MAX_NEW_TOKENS = 256

print("=" * 60)
print("ğŸš€ SahabatAI Simple Inference")
print("=" * 60)
print(f"ğŸ“¦ Model: {MODEL_ID}")
print(f"ğŸ“ Max Tokens: {MAX_NEW_TOKENS}")
print("=" * 60)

# Initialize pipeline
print("\nğŸ¤– Loading model...")
pipeline = transformers.pipeline(
    "text-generation",
    model=MODEL_ID,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

print("âœ… Model loaded successfully!")

# =========================
# EXAMPLE 1: Bahasa Indonesia
# =========================
print("\n" + "=" * 60)
print("ğŸ“ Example 1: Bahasa Indonesia")
print("=" * 60)

messages = [
    {"role": "user", "content": "Apa itu Pancasila?"}
]

print(f"ğŸ‘¤ User: {messages[0]['content']}")
print("ğŸ’­ Generating response...")

outputs = pipeline(
    messages,
    max_new_tokens=MAX_NEW_TOKENS,
    eos_token_id=terminators,
)

response = outputs[0]["generated_text"][-1]
print(f"\nğŸ¤– Assistant ({response['role']}): {response['content']}")

# =========================
# EXAMPLE 2: Javanese
# =========================
print("\n" + "=" * 60)
print("ğŸ“ Example 2: Javanese")
print("=" * 60)

messages = [
    {"role": "user", "content": "Sopo wae sing ana ing Punakawan?"}
]

print(f"ğŸ‘¤ User: {messages[0]['content']}")
print("ğŸ’­ Generating response...")

outputs = pipeline(
    messages,
    max_new_tokens=MAX_NEW_TOKENS,
    eos_token_id=terminators,
)

response = outputs[0]["generated_text"][-1]
print(f"\nğŸ¤– Assistant ({response['role']}): {response['content']}")

# =========================
# EXAMPLE 3: Sundanese
# =========================
print("\n" + "=" * 60)
print("ğŸ“ Example 3: Sundanese")
print("=" * 60)

messages = [
    {"role": "user", "content": "Kumaha caritana si Kabayan?"}
]

print(f"ğŸ‘¤ User: {messages[0]['content']}")
print("ğŸ’­ Generating response...")

outputs = pipeline(
    messages,
    max_new_tokens=MAX_NEW_TOKENS,
    eos_token_id=terminators,
)

response = outputs[0]["generated_text"][-1]
print(f"\nğŸ¤– Assistant ({response['role']}): {response['content']}")

# =========================
# EXAMPLE 4: Multiturn Conversation
# =========================
print("\n" + "=" * 60)
print("ğŸ“ Example 4: Multiturn Conversation")
print("=" * 60)

messages = [
    {"role": "user", "content": "Siapa presiden pertama Indonesia?"},
]

print(f"ğŸ‘¤ User: {messages[0]['content']}")
print("ğŸ’­ Generating response...")

outputs = pipeline(
    messages,
    max_new_tokens=MAX_NEW_TOKENS,
    eos_token_id=terminators,
)

response1 = outputs[0]["generated_text"][-1]
print(f"\nğŸ¤– Assistant: {response1['content']}")

# Add to conversation
messages.append(response1)
messages.append({"role": "user", "content": "Kapan beliau lahir?"})

print(f"\nğŸ‘¤ User: {messages[-1]['content']}")
print("ğŸ’­ Generating response...")

outputs = pipeline(
    messages,
    max_new_tokens=MAX_NEW_TOKENS,
    eos_token_id=terminators,
)

response2 = outputs[0]["generated_text"][-1]
print(f"\nğŸ¤– Assistant: {response2['content']}")

print("\n" + "=" * 60)
print("âœ… All examples completed!")
print("=" * 60)
